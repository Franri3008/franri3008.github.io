{
    "central": {
      "name": "HUGGING FACE ðŸ¤—",
      "subname": "DATASETS",
      "color": "#111111",
      "nameSizeFactor": 0.20,
      "subnameSizeFactor": 1.65,
      "centralCircleSize": 0.40,
      "smallCircleSize": 0.14,
      "icon_transform": "None",
      "top": 0,
      "size_increase": 1.32
    },
    "items": [
      {
        "name": "orca-agentinstruct-1M-v1",
        "subname": "Microsoft",
        "bullets": ["Designed to train models for instruction-following tasks", "Prompts and responses are synthetically generated"],
        "description": "Designed to train models for instruction-following tasks like text creative writing, coding or reading comprehension. Both the prompts and the responses of this dataset are synthetically generated by AgentInstruct, using only raw text content publicly avialble on the Web as seeds.\nNumber of rows: 1,046,410\nSize of auto-converted Parquet files: 2.21 GB",
        "icon": "https://cdn-avatars.huggingface.co/v1/production/uploads/1583646260758-5e64858c87403103f9f1055d.png",
        "url": "https://huggingface.co/datasets/microsoft/orca-agentinstruct-1M-v1",
        "color": "#5086BC",
        "x": 0.0,
        "y": -0.23
      },
      {
        "name": "arXiver",
        "subname": "Neuralwork",
        "bullets": ["Curated for question-answering tasks", "Data is converted to the highly readable (.mmd) format"],
        "description": "The largest open and permissible licensed text dataset, comprising over 2 trillion tokens (2,003,039,184,047 tokens). Contains a diverse set of sources such as books, newspapers, scientific articles, government and legal documents, code, and more.\nEstimated number of rows: 396,953,971\nSize of auto-converted Parquet files (First 5GB): 2.96 GB\nKey feature: Data is permissively licensed, meaning it can be used, modified, and redistributed without legal ambiguity or risk of infringement.",
        "icon": "https://cdn-avatars.huggingface.co/v1/production/uploads/6329b0cabdb6242b42b8cd63/7T7rS_-BL7wLWMDiCZgs7.png",
        "url": "https://huggingface.co/datasets/neuralwork/arxiver",
        "color": "#2FAD3B",
        "x": 0.03,
        "y": -0.03
      },
      {
        "name": "Common Corpus",
        "subname": "PleIAs",
        "bullets": ["Permissible licensed text dataset", "Contains a diverse set of sources (books, newspapers, scientific articles)"],
        "description": "The largest open and permissible licensed text dataset, comprising over 2 trillion tokens (2,003,039,184,047 tokens). Contains a diverse set of sources such as books, newspapers, scientific articles, government and legal documents, code, and more.\nEstimated number of rows: 396,953,971\nSize of auto-converted Parquet files (First 5GB): 2.96 GB\nKey feature: Data is permissively licensed, meaning it can be used, modified, and redistributed without legal ambiguity or risk of infringement.",
        "icon": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ce091a9e9ca8123d7a42b0/vCajdXlzRs0zAU-b_KJ_G.png",
        "url": "https://huggingface.co/datasets/PleIAs/common_corpus",
        "color": "#0221b9",
        "x": 0.05,
        "y": 0.04
      },
      {
        "name": "SmolTalkDataset",
        "subname": "HuggingFaceTB",
        "bullets": ["Designed for supervised finetuning (SFT) of LLMs", "Curated to strengthen model capabilities like mathematics and coding"],
        "description": "Synthetic dataset designed for supervised finetuning (SFT) of LLMs. It focuses on bridging the performance gap between models trained on SFT datasets and those trained on proprietary instruction datasets.\nNumber of rows: 2,197,730\nSize of the auto-converted Parquet files: 4.15 GB\nKey feature: While curated for SFT, the dataset also aims at improving on instruction following tasks.",
        "icon": "https://huggingface.co/front/assets/huggingface_logo-noborder.svg",
        "url": "https://huggingface.co/datasets/HuggingFaceTB/smoltalk",
        "color": "#ffd11e",
        "x": 0.0,
        "y": 0.03
      },
      {
        "name": "MMMLU",
        "subname": "OpenAI",
        "bullets": ["Covers 57 topics from physics to history", "Test set translated into 14 languages using professional human translators"],
        "description": "Benchmark dataset for assessing the general knowledge and reasoning skills of AI models. It covers 57 topics from physics to history, and its test set is translated into 14 languages by professional translators to improve multilingual AI performance and accuracy.\nNumber of rows: 393,176\nSize of the auto-converted Parquet files: 124 MB\nKey feature: Aiming to improve the multilingual capabilities of AI models, ensuring they perform accurately across languages, particularly for underrepresented communities.",
        "icon": "https://cdn-avatars.huggingface.co/v1/production/uploads/1620805164087-5ec0135ded25d76864d553f1.png",
        "url": "https://huggingface.co/datasets/openai/MMMLU",
        "color": "#323185",
        "x": 0.0,
        "y": 0.0
      },
      {
        "name": "FinePersonas",
        "subname": "Argilla",
        "bullets": ["Designed to train models in tasks like reasoning or creative writing", "Enhances the diversity and specificity of synthetic outputs by using diverse personas"],
        "description": "Allows AI researchers and engineers to easily integrate unique persona traits into text generation systems, thereby enhancing the diversity and specificity of synthetic outputs without the complexity of crafting detailed attributes from scratch.\nNumber of rows: 42,142,456\nSize of the auto-converted Parquet files: 143 GB\nKey feature: Focus on providing personas with specific expertise, career paths, or personal interestswhich allow for more nuanced and targeted content.",
        "icon": "https://cdn-avatars.huggingface.co/v1/production/uploads/1664307416166-60420dccc15e823a685f2b03.png",
        "url": "https://huggingface.co/datasets/argilla/FinePersonas-v0.1",
        "color": "#EB321B",
        "x": 0.0,
        "y": -0.07
      },
      {
        "name": "Infinity Instruct",
        "subname": "Beijing Academy of Artificial Intelligence (BAAI)",
        "bullets": ["Large-scale instruction dataset", "Curated to advance open-source model fine-tuning"],
        "description": "A high quality, scalable instruction dataset designed to advance open-source model fine-tuning and research.\nNumber of rows: 7,449,106",
        "icon": "https://cdn-avatars.huggingface.co/v1/production/uploads/1664511063789-632c234f42c386ebd2710434.png",
        "url": "https://huggingface.co/datasets/BAAI/Infinity-Instruct",
        "color": "#8940A8",
        "x": 0.0,
        "y": -0.16
      },
      {
        "name": "TxT360",
        "subname": "LLM360",
        "bullets": ["Globally filters 99 Common Crawl snapshots", "Curated for LLM pretraining and data weighting"],
        "description": "The first dataset to globally deduplicate 99 CommonCrawl snapshots and 14 commonly used non-web data sources, thereby offering scalable, high-quality data for pretraining performant models.",
        "icon": "https://cdn-avatars.huggingface.co/v1/production/uploads/644bf65522d211df6444a7f4/7B9Y9bkBn6K1NDwlZH3dx.png",
        "url": "https://huggingface.co/datasets/LLM360/TxT360",
        "color": "#E95C00",
        "x": 0.05,
        "y": -0.18
      },
      {
        "name": "FineVideo",
        "subname": "HuggingFaceFV",
        "bullets": ["Curated to fine-tune AI for video understanding tasks", "Focuses on of mood analysis and storytelling in multimodal settings"],
        "description": "Focuses on capturing the emotional journey and narrative flow of videos, thereby giving researchers the ingredients to cook up more context-savvy video analysis models.",
        "icon": "https://huggingface.co/front/assets/huggingface_logo-noborder.svg",
        "url": "https://huggingface.co/datasets/HuggingFaceFV/finevideo",
        "color": "#ffd11e",
        "x": 0.08,
        "y": -0.18
      },
      {
        "name": "FineWeb-Edu",
        "subname": "HuggingFaceFW",
        "bullets": ["Collection of high-quality educational web content, filtered from FineWeb", "Data is filtered with an educational classifier for high-quality data"],
        "description": "A 1.3T-token dataset of educational web pages filtered from FineWeb.\nNumber of rows: 3,004,505,493\nSize of the auto-converted Parquet files: 8.84 TB\nKey feature: The dataset uses an AI-trained educational classifier to retain only the most educational web pages.",
        "icon": "https://huggingface.co/front/assets/huggingface_logo-noborder.svg",
        "url": "https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu",
        "color": "#ffd11e",
        "x": 0.12,
        "y": -0.08
      },
      {
        "name": "PersonaHub",
        "subname": "proj-persona",
        "bullets": ["Collection of synthetic personas curated from web data", "Curated for tasks like creative storytelling or game NPC design"],
        "description": "Contains ~1B personas curated from web data, enabling synthetic data creation for tasks like reasoning, instructions, and game NPCs.\nNumber of rows: 375,000\nSize of the auto-converted Parquet files: 102 MB\nKey feature: Providing a large amount of personas allows for scalable, flexible, and easy to use data creation.",
        "icon": "https://huggingface.co/avatars/74566faa9b26f5852c29d1eff65383dc.svg",
        "url": "https://huggingface.co/datasets/proj-persona/PersonaHub",
        "color": "#48AB73",
        "x": 0.0,
        "y": 0.0
      },
      {
        "name": "reasoning-base-20k",
        "subname": "KingNish",
        "bullets": ["Designed to train reasoning models", "Problems are formatted with Chain of Thought explanation"],
        "description": "A dataset designed to train reasoning models to think through complex problems with detailed chains of thought and answers, for refined, human-like reasoning.\nNumber of rows: 19,944\nSize of the auto-converted Parquet files: 118 MB\nKey feature: Focus on complex reasoning problems presented in a Chain of Thought (CoT) format. ",
        "icon": "https://cdn-avatars.huggingface.co/v1/production/uploads/6612aedf09f16e7347dfa7e1/bPYjBXCedY_1fSIPjoBTY.jpeg",
        "url": "https://huggingface.co/datasets/KingNish/reasoning-base-20k",
        "color": "#EA4A7D",
        "x": -0.1,
        "y": -0.1
      },
      {
        "name": "xlam-function-calling-60k",
        "subname": "Salesforce",
        "bullets": ["Verified data for function-calling applications", "Ensures a 95% correctness rate "],
        "description": "Provides high-quality, verified data for function-calling applications, with a 95% correctness rate ensured through rigorous hierarchical verification.\nKey feature: Data is verified at three hierarchical levels: Format verification, actual function execution and semantic verification, ensuring its reliability and correctness.",
        "icon": "https://cdn-avatars.huggingface.co/v1/production/uploads/1602756670970-noauth.jpeg",
        "url": "https://huggingface.co/datasets/Salesforce/xlam-function-calling-60k",
        "color": "#6182C4",
        "x": -0.05,
        "y": -0.2
      },
      {
        "name": "FRAMES",
        "subname": "Google",
        "bullets": ["Designed to evaluate reasoning and factual retrieval capabilities in LLMs", "Questions span diverse topics from history to science"],
        "description": "A comprehensive evaluation dataset designed to test the capabilities of Retrieval-Augmented Generation (RAG) systems across factuality, retrieval accuracy, and reasoning.\nNumber of rows: 824\nSize of the auto-converted Parquet files: 236 kB\nKey feature:",
        "icon": "https://cdn-avatars.huggingface.co/v1/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png",
        "url": "https://huggingface.co/datasets/google/frames-benchmark",
        "color": "#70A1D1",
        "x": 0.0,
        "y": -0.2
      },
      {
        "name": "HelpSteer2",
        "subname": "Nvidia",
        "bullets": ["Designed to train models to be more helpful, factually accurate, and coherent", "Includes feedback on attributes like helpfulness, correctness and verbosity"],
        "description": "An open-source Helpfulness Dataset (CC-BY-4.0) that supports aligning models to become more helpful, factually correct and coherent, while being adjustable in terms of the complexity and verbosity of its responses.",
        "icon": "https://cdn-avatars.huggingface.co/v1/production/uploads/1613114437487-60262a8e0703121c822a80b6.png",
        "url": "https://huggingface.co/datasets/nvidia/HelpSteer2",
        "color": "#2fad3b",
        "x": 0.0,
        "y": -0.14
      },
      {
        "name": "FineWeb 2",
        "subname": "HuggingFaceFW",
        "bullets": ["Built from CommonCrawl snapshots", "Curated to remove duplicates and low-quality content"],
        "description": "Second iteration of the FineWeb dataset.\nEstimated number of rows: 13,782,000,384\nSize of the auto-converted Parquet files (First 5GB per split): 19.4 TB\nKey feature: Curated dataset of diverse, high-quality web documents designed for fine-tuning language models on complex and nuanced tasks",
        "icon": "https://huggingface.co/front/assets/huggingface_logo-noborder.svg",
        "url": "https://huggingface.co/datasets/HuggingFaceFW/fineweb-2",
        "color": "#ffd11e",
        "x": 0.0,
        "y": -0.05
      },
      {
        "name": "two-million-bluesky-posts",
        "subname": "Alpin Dale",
        "bullets": ["2 million public posts collected from Bluesky Social's firehose API", "Designed for studying social media behavior, natural language processing and online communication trends"],
        "description": "2 Million Bluesky Posts\nNumber of rows: 2,107,530\nSize of the auto-converted Parquet files: 380 MB\nKey feature: With 2 million posts from Bluesky, this large-scale social media dataset offers rich insights into social media language and trends",
        "icon": "https://cdn-avatars.huggingface.co/v1/production/uploads/635567189c72a7e742f1419c/tbfBz0furS-y4ISgoe6j0.jpeg",
        "url": "https://huggingface.co/datasets/alpindale/two-million-bluesky-posts",
        "color": "#FF4CF3",
        "x": 0.0,
        "y": 0.02
      },
      {
        "name": "OpenO1-SFT",
        "subname": "O1-OPEN",
        "bullets": ["Designed for fine-tuning language models", "Focuses on natural language understanding and question answering"],
        "description": "Number of rows: 77,685\nSize of the auto-converted Parquet files: 183 MB\nKey feature: Instruction-tuned dataset. Focused on supervised fine-tuning (SFT), featuring diverse tasks and instructions to improve model alignment and usability.",
        "icon": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d2251f98b4a470bf6a26e3/4yeCUW8LBIlAP_gD19i0k.jpeg",
        "url": "https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT/tree/main",
        "color": "#6BC1C4",
        "x": 0.0,
        "y": 0.05
      },
      {
        "name": "5CD-AILLaVA-CoT-o1-Instruct",
        "subname": "5CD-AI",
        "bullets": [""],
        "description": "Focuses on training models to generate step-by-step reasoning processes, enhancing their ability to handle complex, multi-step tasks.",
        "icon": "https://cdn-avatars.huggingface.co/v1/production/uploads/6336b5c831efcb5647f00170/Dj0MUGFuRKcRbzGwB8cVE.png",
        "url": "https://huggingface.co/datasets/5CD-AI/LLaVA-CoT-o1-Instruct",
        "color": "#323185",
        "x": 0.0,
        "y": 0.1
      },
      {
        "name": "Cosmopedia",
        "subname": "HuggingFaceTB",
        "bullets": ["Designed for training LMs on tasks like summarization and educational content generation", "Includes textbooks, blog posts and WikiHow articles"],
        "description": "A 25B-token synthetic dataset of textbooks, blogs, and stories, mapping world knowledge from web datasets like RefinedWeb for research in synthetic data.",
        "icon": "https://huggingface.co/front/assets/huggingface_logo-noborder.svg",
        "url": "https://huggingface.co/datasets/HuggingFaceTB/cosmopedia",
        "color": "#ffd11e",
        "x": -0.05,
        "y": -0.02
      }
    ]
  }
  