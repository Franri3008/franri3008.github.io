[
  {
    "id": "fka/awesome-chatgpt-prompts",
    "name": "fka",
    "parent": "text",
    "color": "#FFD21E",
    "value": 8006,
    "downloads_all_time": 188184,
    "downloads": 22907,
    "likes": 8006,
    "description": "üß† Awesome ChatGPT Prompts [CSV dataset]This is a Dataset Repository of Awesome ChatGPT PromptsView All Prompts on GitHubLicenseCC-0"
  },
  {
    "id": "HuggingFaceFW/fineweb",
    "name": "HuggingFaceFW",
    "parent": "tabular",
    "color": "#106C4E",
    "value": 2203,
    "downloads_all_time": 3723068,
    "downloads": 209694,
    "likes": 2203,
    "description": "üç∑ FineWeb    15 trillion tokens of the finest data the üåê web has to offerWhat is it?The üç∑ FineWeb dataset consists of more than 15T tokens of cleaned and deduplicated english web data from CommonCrawl. The data processing pipeline is optimized for LLM performance and ran on the üè≠ datatrove library, our large scale data processing library. üç∑ FineWeb was originally meant to be a fully open replication of ü¶Ö RefinedWeb, with a release of the full dataset under‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HuggingFaceFW/fineweb."
  },
  {
    "id": "Open-Orca/OpenOrca",
    "name": "Open-Orca",
    "parent": "text",
    "color": "#FFD21E",
    "value": 1413,
    "downloads_all_time": 385213,
    "downloads": 8990,
    "likes": 1413,
    "description": "üêã The OpenOrca Dataset! üêãWe are thrilled to announce the release of the OpenOrca dataset!This rich collection of augmented FLAN data aligns, as best as possible, with the distributions outlined in the Orca paper.It has been instrumental in generating high-performing model checkpoints and serves as a valuable resource for all NLP researchers and developers!Official ModelsMistral-7B-OpenOrcaOur latest model, the first 7B to score better overall than all‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Open-Orca/OpenOrca."
  },
  {
    "id": "OpenAssistant/oasst1",
    "name": "OpenAssistant",
    "parent": "tabular",
    "color": "#106C4E",
    "value": 1399,
    "downloads_all_time": 272800,
    "downloads": 6249,
    "likes": 1399,
    "description": "OpenAssistant Conversations Dataset (OASST1)Dataset SummaryIn an effort to democratize research on large-scale alignment, we release OpenAssistant Conversations (OASST1), a human-generated, human-annotated assistant-style conversation corpus consisting of 161,443 messages in 35 different languages, annotated with 461,292 quality ratings, resulting in over 10,000 fully annotated conversation trees. The corpus is a product of a worldwide crowd-sourcing effort‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/OpenAssistant/oasst1."
  },
  {
    "id": "Anthropic/hh-rlhf",
    "name": "Anthropic",
    "parent": "text",
    "color": "#FFD21E",
    "value": 1364,
    "downloads_all_time": 1598842,
    "downloads": 11040,
    "likes": 1364,
    "description": "Dataset Card for HH-RLHFDataset SummaryThis repository provides access to two different kinds of data:Human preference data about helpfulness and harmlessness from Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback. These data are meant to train preference (or reward) models for subsequent RLHF training. These data are not meant for supervised training of dialogue agents. Training dialogue agents on these data is likely to lead‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Anthropic/hh-rlhf."
  },
  {
    "id": "gsdf/EasyNegative",
    "name": "gsdf",
    "parent": "image",
    "color": "#FF9D00",
    "value": 1149,
    "downloads_all_time": 308547,
    "downloads": 24071,
    "likes": 1149,
    "description": "Negative EmbeddingThis is a Negative Embedding trained with Counterfeit. Please use it in the \"\\stable-diffusion-webui\\embeddings\" folder.It can be used with other models, but the effectiveness is not certain.  Counterfeit-V2.0.safetensorsAbyssOrangeMix2_sfw.safetensorsanything-v4.0-pruned.safetensors"
  },
  {
    "id": "togethercomputer/RedPajama-Data-1T",
    "name": "togethercomputer",
    "parent": "text",
    "color": "#FFD21E",
    "value": 1092,
    "downloads_all_time": 433688,
    "downloads": 1991,
    "likes": 1092,
    "description": "RedPajama is a clean-room, fully open-source implementation of the LLaMa dataset."
  },
  {
    "id": "Nerfgun3/bad_prompt",
    "name": "Nerfgun3",
    "parent": "image",
    "color": "#FF9D00",
    "value": 925,
    "downloads_all_time": 56852,
    "downloads": 4405,
    "likes": 925,
    "description": "Negative Embedding / Textual InversionIdeaThe idea behind this embedding was to somehow train the negative prompt as an embedding, thus unifying the basis of the negative prompt into one word or embedding. Side note: Embedding has proven to be very helpful for the generation of hands! :)UsageTo use this embedding you have to download the file aswell as drop it into the \"\\stable-diffusion-webui\\embeddings\" folder.Please put the embedding in the negative‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Nerfgun3/bad_prompt."
  },
  {
    "id": "allenai/dolma",
    "name": "allenai",
    "parent": "unknown",
    "color": "#6B7280",
    "value": 912,
    "downloads_all_time": 356713,
    "downloads": 1796,
    "likes": 912,
    "description": "Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research"
  },
  {
    "id": "tiiuae/falcon-refinedweb",
    "name": "tiiuae",
    "parent": "text",
    "color": "#FFD21E",
    "value": 857,
    "downloads_all_time": 752693,
    "downloads": 10055,
    "likes": 857,
    "description": "üìÄ Falcon RefinedWebFalcon RefinedWeb is a massive English web dataset built by TII and released under an ODC-By 1.0 license.See the üìì paper on arXiv for more details. RefinedWeb is built through stringent filtering and large-scale deduplication of CommonCrawl; we found models trained on RefinedWeb to achieve performance in-line or better than models trained on curated datasets, while only relying on web data. RefinedWeb is also \"multimodal-friendly\": it contains links and alt‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tiiuae/falcon-refinedweb."
  },
  {
    "id": "wikimedia/wikipedia",
    "name": "wikimedia",
    "parent": "text",
    "color": "#FFD21E",
    "value": 847,
    "downloads_all_time": 1218765,
    "downloads": 74863,
    "likes": 847,
    "description": "Dataset Card for Wikimedia WikipediaDataset SummaryWikipedia dataset containing cleaned articles of all languages.The dataset is built from the Wikipedia dumps (https://dumps.wikimedia.org/)with one subset per language, each containing a single train split.Each example contains the content of one full Wikipedia article with cleaning to stripmarkdown and unwanted sections (references, etc.).All language subsets have already been processed for recent dump, and you‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/wikimedia/wikipedia."
  },
  {
    "id": "bigcode/the-stack",
    "name": "bigcode",
    "parent": "tabular",
    "color": "#106C4E",
    "value": 829,
    "downloads_all_time": 202471,
    "downloads": 13979,
    "likes": 829,
    "description": "Dataset Card for The StackChangelogReleaseDescriptionv1.0Initial release of the Stack. Included 30 programming languages and 18 permissive licenses. Note: Three included licenses (MPL/EPL/LGPL) are considered weak copyleft licenses. The resulting near-deduplicated dataset is 3TB in size.v1.1The three copyleft licenses ((MPL/EPL/LGPL) were excluded and the list of permissive licenses extended to 193 licenses in total. The list of programming languages‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/bigcode/the-stack."
  },
  {
    "id": "databricks/databricks-dolly-15k",
    "name": "databricks",
    "parent": "text",
    "color": "#FFD21E",
    "value": 826,
    "downloads_all_time": 635978,
    "downloads": 13408,
    "likes": 826,
    "description": "Summarydatabricks-dolly-15k is an open source dataset of instruction-following records generated by thousands of Databricks employees in several of the behavioral categories outlined in the InstructGPT paper, including brainstorming, classification, closed QA, generation, information extraction, open QA, and summarization.This dataset can be used for any purpose, whether academic or commercial,  under the terms of the Creative Commons Attribution-ShareAlike 3.0 Unported‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/databricks/databricks-dolly-15k."
  },
  {
    "id": "anon8231489123/ShareGPT_Vicuna_unfiltered",
    "name": "anon8231489123",
    "parent": "unknown",
    "color": "#6B7280",
    "value": 800,
    "downloads_all_time": 239137,
    "downloads": 43191,
    "likes": 800,
    "description": "Further cleaning done. Please look through the dataset and ensure that I didn't miss anything.Update: Confirmed working method for training the model: https://huggingface.co/AlekseyKorshuk/vicuna-7b/discussions/4#64346c08ef6d5abefe42c12cTwo choices:Removes instances of \"I'm sorry, but\": https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/blob/main/ShareGPT_V3_unfiltered_cleaned_split_no_imsorry.jsonHas instances of \"I'm sorry, but\":‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered."
  },
  {
    "id": "openai/gsm8k",
    "name": "openai",
    "parent": "text",
    "color": "#FFD21E",
    "value": 776,
    "downloads_all_time": 5709237,
    "downloads": 510808,
    "likes": 776,
    "description": "Dataset Card for GSM8KDataset SummaryGSM8K (Grade School Math 8K) is a dataset of 8.5K high quality linguistically diverse grade school math word problems. The dataset was created to support the task of question answering on basic mathematical problems that require multi-step reasoning.These problems take between 2 and 8 steps to solve.Solutions primarily involve performing a sequence of elementary calculations using basic arithmetic operations (+ ‚àí √ó√∑) to reach the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/openai/gsm8k."
  },
  {
    "id": "tatsu-lab/alpaca",
    "name": "tatsu-lab",
    "parent": "text",
    "color": "#FFD21E",
    "value": 774,
    "downloads_all_time": 1412075,
    "downloads": 37938,
    "likes": 774,
    "description": "Dataset Card for AlpacaDataset SummaryAlpaca is a dataset of 52,000 instructions and demonstrations generated by OpenAI's text-davinci-003 engine. This instruction data can be used to conduct instruction-tuning for language models and make the language model follow instruction better.The authors built on the data generation pipeline from Self-Instruct framework and made the following modifications:The text-davinci-003 engine to generate the instruction data instead‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/tatsu-lab/alpaca."
  },
  {
    "id": "FreedomIntelligence/medical-o1-reasoning-SFT",
    "name": "FreedomIntelligence",
    "parent": "text",
    "color": "#FFD21E",
    "value": 753,
    "downloads_all_time": 80178,
    "downloads": 9238,
    "likes": 753,
    "description": "News[2025/04/22] We split the data and kept only the medical SFT dataset (medical_o1_sft.json). The file medical_o1_sft_mix.json contains a mix of medical and general instruction data.[2025/02/22] We released the distilled dataset from Deepseek-R1 based on medical verifiable problems. You can use it to initialize your models with the reasoning chain from Deepseek-R1.[2024/12/25] We open-sourced the medical reasoning dataset for SFT, built on medical verifiable problems and an LLM‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/FreedomIntelligence/medical-o1-reasoning-SFT."
  },
  {
    "id": "teknium/OpenHermes-2.5",
    "name": "teknium",
    "parent": "text",
    "color": "#FFD21E",
    "value": 733,
    "downloads_all_time": 133728,
    "downloads": 3195,
    "likes": 733,
    "description": "Dataset Card for Dataset NameThis is the dataset that made OpenHermes 2.5 and Nous Hermes 2 series of models.Support me on GitHub sponsors <3 : https://github.com/sponsors/teknium1Dataset DetailsDataset DescriptionThe Open Hermes 2/2.5 and Nous Hermes 2 models have made significant advancements of SOTA LLM's over recent months, and are underpinned by this exact compilation and curation of many open source datasets and custom created synthetic datasets.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/teknium/OpenHermes-2.5."
  },
  {
    "id": "QingyiSi/Alpaca-CoT",
    "name": "QingyiSi",
    "parent": "unknown",
    "color": "#6B7280",
    "value": 731,
    "downloads_all_time": 143955,
    "downloads": 36198,
    "likes": 731,
    "description": "Instruction-Finetuning Dataset Collection (Alpaca-CoT)This repository will continuously collect various instruction tuning datasets. And we standardize different datasets into the same format, which can be directly loaded by the code of Alpaca model.We also have conducted empirical study on various instruction-tuning datasets based on the Alpaca model, as shown in https://github.com/PhoebusSi/alpaca-CoT.  If you think this dataset collection is helpful to you, please like‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/QingyiSi/Alpaca-CoT."
  },
  {
    "id": "open-thoughts/OpenThoughts-114k",
    "name": "open-thoughts",
    "parent": "text",
    "color": "#FFD21E",
    "value": 714,
    "downloads_all_time": 228559,
    "downloads": 40080,
    "likes": 714,
    "description": "    [!NOTE]We have released a paper for OpenThoughts! See our paper here. Open-Thoughts-114kOpen synthetic reasoning dataset with 114k high-quality examples covering math, science, code, and puzzles!Inspect the content with rich formatting with Curator Viewer.Available Subsetsdefault subset containing ready-to-train data used to finetune the OpenThinker-7B and OpenThinker-32B models:ds = load_dataset(\"open-thoughts/OpenThoughts-114k\", split=\"train\")‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k."
  },
  {
    "id": "HuggingFaceFW/fineweb-edu",
    "name": "HuggingFaceFW",
    "parent": "tabular",
    "color": "#106C4E",
    "value": 698,
    "downloads_all_time": 3651729,
    "downloads": 118343,
    "likes": 698,
    "description": "üìö FineWeb-Edu    1.3 trillion tokens of the finest educational data the üåê web has to offerPaper: https://arxiv.org/abs/2406.17557What is it?üìö FineWeb-Edu  dataset consists of 1.3T tokens  and  5.4T tokens (FineWeb-Edu-score-2) of educational web pages filtered from üç∑ FineWeb dataset. This is the 1.3 trillion version.To enhance FineWeb's quality, we developed an educational quality classifier using annotations generated by LLama3-70B-Instruct. We then‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu."
  },
  {
    "id": "yahma/alpaca-cleaned",
    "name": "yahma",
    "parent": "text",
    "color": "#FFD21E",
    "value": 693,
    "downloads_all_time": 684142,
    "downloads": 24865,
    "likes": 693,
    "description": "Dataset Card for Alpaca-CleanedRepository: https://github.com/gururise/AlpacaDataCleanedDataset DescriptionThis is a cleaned version of the original Alpaca Dataset released by Stanford. The following issues have been identified in the original release and fixed in this dataset:Hallucinations: Many instructions in the original dataset had instructions referencing data on the internet, which just caused GPT3 to hallucinate an answer.\"instruction\":\"Summarize the‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/yahma/alpaca-cleaned."
  },
  {
    "id": "roneneldan/TinyStories",
    "name": "roneneldan",
    "parent": "text",
    "color": "#FFD21E",
    "value": 684,
    "downloads_all_time": 682696,
    "downloads": 24879,
    "likes": 684,
    "description": "Dataset containing synthetically generated (by GPT-3.5 and GPT-4) short stories that only use a small vocabulary.Described in the following paper: https://arxiv.org/abs/2305.07759. The models referred to in the paper were trained on TinyStories-train.txt  (the file tinystories-valid.txt can be used for validation loss). These models can be found on Huggingface, at roneneldan/TinyStories-1M/3M/8M/28M/33M/1Layer-21M.Additional resources:tinystories_all_data.tar.gz - contains a superset of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/roneneldan/TinyStories."
  },
  {
    "id": "lmsys/lmsys-chat-1m",
    "name": "lmsys",
    "parent": "text",
    "color": "#FFD21E",
    "value": 682,
    "downloads_all_time": 239174,
    "downloads": 6172,
    "likes": 682,
    "description": "LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation DatasetThis dataset contains one million real-world conversations with 25 state-of-the-art LLMs.It is collected from 210K unique IP addresses in the wild on the Vicuna demo and Chatbot Arena website from April to August 2023.Each sample includes a conversation ID, model name, conversation text in OpenAI API JSON format, detected language tag, and OpenAI moderation API tag.User consent is obtained through the \"Terms of use\"‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/lmsys/lmsys-chat-1m."
  },
  {
    "id": "Congliu/Chinese-DeepSeek-R1-Distill-data-110k",
    "name": "Congliu",
    "parent": "tabular",
    "color": "#106C4E",
    "value": 670,
    "downloads_all_time": 15679,
    "downloads": 1079,
    "likes": 670,
    "description": "‰∏≠ÊñáÂü∫‰∫éÊª°Ë°ÄDeepSeek-R1Ëí∏È¶èÊï∞ÊçÆÈõÜÔºàChinese-Data-Distill-From-R1Ôºâü§ó Hugging Face¬†¬† | ¬†¬†ü§ñ ModelScope ¬†¬† | ¬†¬†üöÄ Github ¬†¬† | ¬†¬†üìë BlogÊ≥®ÊÑèÔºöÊèê‰æõ‰∫ÜÁõ¥Êé•SFT‰ΩøÁî®ÁöÑÁâàÊú¨ÔºåÁÇπÂáª‰∏ãËΩΩ„ÄÇÂ∞ÜÊï∞ÊçÆ‰∏≠ÁöÑÊÄùËÄÉÂíåÁ≠îÊ°àÊï¥ÂêàÊàêoutputÂ≠óÊÆµÔºåÂ§ßÈÉ®ÂàÜSFT‰ª£Á†ÅÊ°ÜÊû∂ÂùáÂèØÁõ¥Êé•Áõ¥Êé•Âä†ËΩΩËÆ≠ÁªÉ„ÄÇÊú¨Êï∞ÊçÆÈõÜ‰∏∫‰∏≠ÊñáÂºÄÊ∫êËí∏È¶èÊª°Ë°ÄR1ÁöÑÊï∞ÊçÆÈõÜÔºåÊï∞ÊçÆÈõÜ‰∏≠‰∏ç‰ªÖÂåÖÂê´mathÊï∞ÊçÆÔºåËøòÂåÖÊã¨Â§ßÈáèÁöÑÈÄöÁî®Á±ªÂûãÊï∞ÊçÆÔºåÊÄªÊï∞Èáè‰∏∫110K„ÄÇ‰∏∫‰ªÄ‰πàÂºÄÊ∫êËøô‰∏™Êï∞ÊçÆÔºüR1ÁöÑÊïàÊûúÂçÅÂàÜÂº∫Â§ßÔºåÂπ∂‰∏îÂü∫‰∫éR1Ëí∏È¶èÊï∞ÊçÆSFTÁöÑÂ∞èÊ®°Âûã‰πüÂ±ïÁé∞Âá∫‰∫ÜÂº∫Â§ßÁöÑÊïàÊûúÔºå‰ΩÜÊ£ÄÁ¥¢ÂèëÁé∞ÔºåÂ§ßÈÉ®ÂàÜÂºÄÊ∫êÁöÑR1Ëí∏È¶èÊï∞ÊçÆÈõÜÂùá‰∏∫Ëã±ÊñáÊï∞ÊçÆÈõÜ„ÄÇ ÂêåÊó∂ÔºåR1ÁöÑÊä•Âëä‰∏≠Â±ïÁ§∫ÔºåËí∏È¶èÊ®°Âûã‰∏≠ÂêåÊó∂‰πü‰ΩøÁî®‰∫ÜÈÉ®ÂàÜÈÄöÁî®Âú∫ÊôØÊï∞ÊçÆÈõÜ„ÄÇ‰∏∫‰∫ÜÂ∏ÆÂä©Â§ßÂÆ∂Êõ¥Â•ΩÂú∞Â§çÁé∞R1Ëí∏È¶èÊ®°ÂûãÁöÑÊïàÊûúÔºåÁâπÊ≠§ÂºÄÊ∫ê‰∏≠ÊñáÊï∞ÊçÆÈõÜ„ÄÇËØ•‰∏≠ÊñáÊï∞ÊçÆÈõÜ‰∏≠ÁöÑÊï∞ÊçÆÂàÜÂ∏ÉÂ¶Ç‰∏ãÔºöMathÔºöÂÖ±ËÆ°36568‰∏™Ê†∑Êú¨ÔºåExamÔºöÂÖ±ËÆ°2432‰∏™Ê†∑Êú¨ÔºåSTEMÔºöÂÖ±ËÆ°12648‰∏™Ê†∑Êú¨Ôºå‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Congliu/Chinese-DeepSeek-R1-Distill-data-110k."
  },
  {
    "id": "BAAI/Infinity-Instruct",
    "name": "BAAI",
    "parent": "tabular",
    "color": "#106C4E",
    "value": 639,
    "downloads_all_time": 59310,
    "downloads": 2768,
    "likes": 639,
    "description": "Infinity InstructBeijing Academy of Artificial Intelligence (BAAI) [Paper][Code][ü§ó] The quality and scale of instruction data are crucial for model performance. Recently, open-source models have increasingly relied on fine-tuning datasets comprising millions of instances, necessitating both high quality and large scale. However, the open-source community has long been constrained by the high costs associated with building such extensive and high-quality instruction‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/BAAI/Infinity-Instruct."
  },
  {
    "id": "m-a-p/COIG-CQIA",
    "name": "m-a-p",
    "parent": "text",
    "color": "#FFD21E",
    "value": 635,
    "downloads_all_time": 53572,
    "downloads": 5250,
    "likes": 635,
    "description": "              COIG-CQIAÔºöQuality is All you need for Chinese Instruction Fine-tuningDataset DetailsDataset DescriptionÊ¨¢ËøéÊù•Âà∞COIG-CQIAÔºåCOIG-CQIAÂÖ®Áß∞‰∏∫Chinese Open Instruction Generalist - Quality is All You NeedÔºå ÊòØ‰∏Ä‰∏™ÂºÄÊ∫êÁöÑÈ´òË¥®ÈáèÊåá‰ª§ÂæÆË∞ÉÊï∞ÊçÆÈõÜÔºåÊó®Âú®‰∏∫‰∏≠ÊñáNLPÁ§æÂå∫Êèê‰æõÈ´òË¥®Èáè‰∏îÁ¨¶Âêà‰∫∫Á±ª‰∫§‰∫íË°å‰∏∫ÁöÑÊåá‰ª§ÂæÆË∞ÉÊï∞ÊçÆ„ÄÇCOIG-CQIA‰ª•‰∏≠Êñá‰∫íËÅîÁΩëËé∑ÂèñÂà∞ÁöÑÈóÆÁ≠îÂèäÊñáÁ´†‰Ωú‰∏∫ÂéüÂßãÊï∞ÊçÆÔºåÁªèËøáÊ∑±Â∫¶Ê∏ÖÊ¥ó„ÄÅÈáçÊûÑÂèä‰∫∫Â∑•ÂÆ°Ê†∏ÊûÑÂª∫ËÄåÊàê„ÄÇÊú¨È°πÁõÆÂèóLIMA: Less Is More for AlignmentÁ≠âÁ†îÁ©∂ÂêØÂèëÔºå‰ΩøÁî®Â∞ëÈáèÈ´òË¥®ÈáèÁöÑÊï∞ÊçÆÂç≥ÂèØËÆ©Â§ßËØ≠Ë®ÄÊ®°ÂûãÂ≠¶‰π†Âà∞‰∫∫Á±ª‰∫§‰∫íË°å‰∏∫ÔºåÂõ†Ê≠§Âú®Êï∞ÊçÆÊûÑÂª∫‰∏≠Êàë‰ª¨ÂçÅÂàÜÊ≥®ÈáçÊï∞ÊçÆÁöÑÊù•Ê∫ê„ÄÅË¥®Èáè‰∏éÂ§öÊ†∑ÊÄßÔºåÊï∞ÊçÆÈõÜËØ¶ÊÉÖËØ∑ËßÅÊï∞ÊçÆ‰ªãÁªç‰ª•ÂèäÊàë‰ª¨Êé•‰∏ãÊù•ÁöÑËÆ∫Êñá„ÄÇWelcome to the COIG-CQIA‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/m-a-p/COIG-CQIA."
  },
  {
    "id": "HuggingFaceTB/cosmopedia",
    "name": "HuggingFaceTB",
    "parent": "text",
    "color": "#FFD21E",
    "value": 621,
    "downloads_all_time": 220212,
    "downloads": 5280,
    "likes": 621,
    "description": "Cosmopedia v0.1        Image generated by DALL-E, the prompt was generated by Mixtral-8x7B-Instruct-v0.1Note: Cosmopedia v0.2 is available at smollm-corpusUser: What do you think \"Cosmopedia\" could mean? Hint: in our case it's not related to cosmology.Mixtral-8x7B-Instruct-v0.1: A possible meaning for \"Cosmopedia\" could be an encyclopedia or collection of information aboutdifferent cultures, societies, and topics from around the world, emphasizing diversity and global‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HuggingFaceTB/cosmopedia."
  },
  {
    "id": "legacy-datasets/wikipedia",
    "name": "legacy-datasets",
    "parent": "unknown",
    "color": "#6B7280",
    "value": 596,
    "downloads_all_time": 2069500,
    "downloads": 28715,
    "likes": 596,
    "description": "Wikipedia dataset containing cleaned articles of all languages.The datasets are built from the Wikipedia dump(https://dumps.wikimedia.org/) with one split per language. Each examplecontains the content of one full Wikipedia article with cleaning to stripmarkdown and unwanted sections (references, etc.)."
  },
  {
    "id": "proj-persona/PersonaHub",
    "name": "proj-persona",
    "parent": "text",
    "color": "#FFD21E",
    "value": 594,
    "downloads_all_time": 60445,
    "downloads": 6358,
    "likes": 594,
    "description": "Scaling Synthetic Data Creation with 1,000,000,000 PersonasThis repo releases data introduced in our paper Scaling Synthetic Data Creation with 1,000,000,000 Personas:We propose a novel persona-driven data synthesis methodology that leverages various perspectives within a large language model (LLM) to create diverse synthetic data. To fully exploit this methodology at scale, we introduce PERSONA HUB ‚Äì a collection of 1 billion diverse personas automatically curated from web data.‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/proj-persona/PersonaHub."
  },
  {
    "id": "open-r1/OpenR1-Math-220k",
    "name": "open-r1",
    "parent": "text",
    "color": "#FFD21E",
    "value": 592,
    "downloads_all_time": 156811,
    "downloads": 27220,
    "likes": 592,
    "description": "OpenR1-Math-220kDataset descriptionOpenR1-Math-220k is a large-scale dataset for mathematical reasoning. It consists of 220k math problems with two to four reasoning traces generated by DeepSeek R1 for problems from NuminaMath 1.5. The traces were verified using Math Verify for most samples and Llama-3.3-70B-Instruct as a judge for 12% of the samples, and each problem contains at least one reasoning trace with a correct answer.The dataset consists of two splits:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/open-r1/OpenR1-Math-220k."
  },
  {
    "id": "gretelai/synthetic_text_to_sql",
    "name": "gretelai",
    "parent": "text",
    "color": "#FFD21E",
    "value": 559,
    "downloads_all_time": 53913,
    "downloads": 2993,
    "likes": 559,
    "description": "    Image generated by DALL-E. See prompt for more detailssynthetic_text_to_sqlgretelai/synthetic_text_to_sql is a rich dataset of high quality synthetic Text-to-SQL samples, designed and generated using Gretel Navigator, and released under Apache 2.0.Please see our release blogpost for more details.The dataset includes:  105,851 records partitioned into 100,000 train and 5,851 test records  ~23M total tokens, including ~12M SQL tokens  Coverage across 100 distinct‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/gretelai/synthetic_text_to_sql."
  },
  {
    "id": "liwu/MNBVC",
    "name": "liwu",
    "parent": "unknown",
    "color": "#6B7280",
    "value": 546,
    "downloads_all_time": 176787,
    "downloads": 15893,
    "likes": 546,
    "description": "MNBVC: Massive Never-ending BT Vast Chinese corpus"
  },
  {
    "id": "HuggingFaceH4/ultrachat_200k",
    "name": "HuggingFaceH4",
    "parent": "text",
    "color": "#FFD21E",
    "value": 544,
    "downloads_all_time": 562780,
    "downloads": 17979,
    "likes": 544,
    "description": "Dataset Card for UltraChat 200kDataset DescriptionThis is a heavily filtered version of the UltraChat dataset and was used to train Zephyr-7B-Œ≤, a state of the art 7b chat model.The original datasets consists of 1.4M dialogues generated by ChatGPT and spanning a wide range of topics. To create UltraChat 200k, we applied the following logic:Selection of a subset of data for faster supervised fine tuning.Truecasing of the dataset, as we observed around 5% of the data‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k."
  },
  {
    "id": "liuhaotian/LLaVA-Instruct-150K",
    "name": "liuhaotian",
    "parent": "unknown",
    "color": "#6B7280",
    "value": 522,
    "downloads_all_time": 33503,
    "downloads": 3035,
    "likes": 522,
    "description": "LLaVA Visual Instruct 150K Dataset CardDataset detailsDataset type:LLaVA Visual Instruct 150K is a set of GPT-generated multimodal instruction-following data.It is constructed for visual instruction tuning and for building large multimodal towards GPT-4 vision/language capability.Dataset date:LLaVA Visual Instruct 150K was collected in April 2023, by prompting GPT-4-0314 API.Paper or resources for more information:https://llava-vl.github.io/License:‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K."
  },
  {
    "id": "uonlp/CulturaX",
    "name": "uonlp",
    "parent": "text",
    "color": "#FFD21E",
    "value": 521,
    "downloads_all_time": 301249,
    "downloads": 7561,
    "likes": 521,
    "description": "     CulturaX      Cleaned, Enormous, and Public: The Multilingual Fuel to Democratize Large Language Models for 167 Languages Dataset SummaryWe present CulturaX, a substantial multilingual dataset with 6.3 trillion tokens in 167 languages, tailored for large language model (LLM) development. Our dataset undergoes meticulous cleaning and deduplication through a rigorous pipeline of multiple stages to accomplish the best quality for model training, including language‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/uonlp/CulturaX."
  },
  {
    "id": "ILSVRC/imagenet-1k",
    "name": "ILSVRC",
    "parent": "unknown",
    "color": "#6B7280",
    "value": 518,
    "downloads_all_time": 1149947,
    "downloads": 31383,
    "likes": 518,
    "description": "ILSVRC 2012, commonly known as 'ImageNet' is an image dataset organized according to the WordNet hierarchy. Each meaningful concept in WordNet, possibly described by multiple words or word phrases, is called a \"synonym set\" or \"synset\". There are more than 100,000 synsets in WordNet, majority of them are nouns (80,000+). ImageNet aims to provide on average 1000 images to illustrate each synset. Images of each concept are quality-controlled and human-annotated. In its completion, ImageNet hopes to offer tens of millions of cleanly sorted images for most of the concepts in the WordNet hierarchy. ImageNet 2012 is the most commonly used subset of ImageNet. This dataset spans 1000 object classes and contains 1,281,167 training images, 50,000 validation images and 100,000 test images"
  },
  {
    "id": "nvidia/Llama-Nemotron-Post-Training-Dataset",
    "name": "nvidia",
    "parent": "text",
    "color": "#FFD21E",
    "value": 516,
    "downloads_all_time": 26903,
    "downloads": 9893,
    "likes": 516,
    "description": "Llama-Nemotron-Post-Training-Dataset-v1.1 ReleaseUpdate [4/8/2025]: v1.1: We are releasing an additional 2.2M Math and 500K Code Reasoning Data in support of our release of Llama-3.1-Nemotron-Ultra-253B-v1. üéâ Data OverviewThis dataset is a compilation of SFT and RL data that supports improvements of math, code, general reasoning, and instruction following capabilities of the original Llama instruct model, in support of NVIDIA‚Äôs release of‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nvidia/Llama-Nemotron-Post-Training-Dataset."
  },
  {
    "id": "JosephusCheung/GuanacoDataset",
    "name": "JosephusCheung",
    "parent": "text",
    "color": "#FFD21E",
    "value": 515,
    "downloads_all_time": 14087,
    "downloads": 31,
    "likes": 515,
    "description": "Sorry, it's no longer available on Hugging Face. Please reach out to those who have already downloaded it. If you have a copy, please refrain from re-uploading it to Hugging Face. The people here don't deserve it. See also: https://twitter.com/RealJosephus/status/1779913520529707387GuanacoDatasetNews: We're heading towards multimodal VQA, with blip2-flan-t5-xxl Alignment to Guannaco 7B LLM.Still under construction: GuanacoVQA weight & GuanacoVQA Dataset Notice: Effective‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/JosephusCheung/GuanacoDataset."
  },
  {
    "id": "facebook/natural_reasoning",
    "name": "facebook",
    "parent": "text",
    "color": "#FFD21E",
    "value": 506,
    "downloads_all_time": 23469,
    "downloads": 1349,
    "likes": 506,
    "description": "NaturalReasoning is a large-scale dataset for general reasoning tasks. It consists of high-quality challenging reasoning questions backtranslated from pretraining corpora DCLM and FineMath. The questions have been deduplicated and decontaminated from popular reasoning benchmarks including MATH, GPQA, MMLU-Pro, MMLU-STEM. For each question, we extract the reference final answer from the original document from the pretraining corpora if possible. We also provide a model-generated response from‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/facebook/natural_reasoning."
  },
  {
    "id": "poloclub/diffusiondb",
    "name": "poloclub",
    "parent": "unknown",
    "color": "#6B7280",
    "value": 501,
    "downloads_all_time": 2281572,
    "downloads": 8367,
    "likes": 501,
    "description": "DiffusionDB is the first large-scale text-to-image prompt dataset. It contains 2million images generated by Stable Diffusion using prompts and hyperparametersspecified by real users. The unprecedented scale and diversity of thishuman-actuated dataset provide exciting research opportunities in understandingthe interplay between prompts and generative models, detecting deepfakes, anddesigning human-AI interaction tools to help users more easily use these models."
  },
  {
    "id": "HuggingFaceFW/fineweb-2",
    "name": "HuggingFaceFW",
    "parent": "tabular",
    "color": "#106C4E",
    "value": 495,
    "downloads_all_time": 420375,
    "downloads": 38242,
    "likes": 495,
    "description": "ü•Ç FineWeb2    A sparkling update with 1000s of languagesWhat is it?This is the second iteration of the popular üç∑ FineWeb dataset, bringing high quality pretraining data to over 1000 üó£Ô∏è languages.The ü•Ç FineWeb2 dataset is fully reproducible, available under the permissive ODC-By 1.0 license and extensively validated through hundreds of ablation experiments.In particular, on the set of 9 diverse languages we used to guide our processing decisions, ü•Ç FineWeb2‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HuggingFaceFW/fineweb-2."
  },
  {
    "id": "cais/mmlu",
    "name": "cais",
    "parent": "text",
    "color": "#FFD21E",
    "value": 491,
    "downloads_all_time": 37596417,
    "downloads": 161937,
    "likes": 491,
    "description": "Dataset Card for MMLUDataset SummaryMeasuring Massive Multitask Language Understanding by Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt (ICLR 2021).This is a massive multitask test consisting of multiple-choice questions from various branches of knowledge. The test spans subjects in the humanities, social sciences, hard sciences, and other areas that are important for some people to learn. This covers 57 tasks‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cais/mmlu."
  },
  {
    "id": "Gustavosta/Stable-Diffusion-Prompts",
    "name": "Gustavosta",
    "parent": "text",
    "color": "#FFD21E",
    "value": 488,
    "downloads_all_time": 130805,
    "downloads": 6173,
    "likes": 488,
    "description": "Stable Diffusion DatasetThis is a set of about 80,000 prompts filtered and extracted from the image finder for Stable Diffusion: \"Lexica.art\". It was a little difficult to extract the data, since the search engine still doesn't have a public API without being protected by cloudflare.If you want to test the model with a demo, you can go to: \"spaces/Gustavosta/MagicPrompt-Stable-Diffusion\".If you want to see the model, go to: \"Gustavosta/MagicPrompt-Stable-Diffusion\"."
  },
  {
    "id": "openai/MMMLU",
    "name": "openai",
    "parent": "text",
    "color": "#FFD21E",
    "value": 488,
    "downloads_all_time": 63748,
    "downloads": 5415,
    "likes": 488,
    "description": "Multilingual Massive Multitask Language Understanding (MMMLU)The MMLU is a widely recognized benchmark of general knowledge attained by AI models. It covers a broad range of topics from 57 different categories, covering elementary-level knowledge up to advanced professional subjects like law, physics, history, and computer science.We translated the MMLU‚Äôs test set into 14 languages using professional human translators. Relying on human translators for this evaluation increases‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/openai/MMMLU."
  },
  {
    "id": "HuggingFaceH4/no_robots",
    "name": "HuggingFaceH4",
    "parent": "text",
    "color": "#FFD21E",
    "value": 483,
    "downloads_all_time": 69634,
    "downloads": 1714,
    "likes": 483,
    "description": "Dataset Card for No Robots üôÖ‚Äç‚ôÇÔ∏èü§ñLook Ma, an instruction dataset that wasn't generated by GPTs!Dataset SummaryNo Robots is a high-quality dataset of 10,000 instructions and demonstrations created by skilled human annotators. This data can be used for supervised fine-tuning (SFT) to make language models follow instructions better. No Robots was modelled after the instruction dataset described in OpenAI's InstructGPT paper, and is comprised mostly of single-turn‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/HuggingFaceH4/no_robots."
  },
  {
    "id": "cerebras/SlimPajama-627B",
    "name": "cerebras",
    "parent": "unknown",
    "color": "#6B7280",
    "value": 476,
    "downloads_all_time": 1294074,
    "downloads": 48786,
    "likes": 476,
    "description": "The dataset consists of 59166 jsonl files and is ~895GB compressed. It is a cleaned and deduplicated version of Together's RedPajama. Check out our blog post explaining our methods, our code on GitHub, and join the discussion on the Cerebras Discord.Getting StartedYou can download the dataset using Hugging Face datasets:from datasets import load_datasetds = load_dataset(\"cerebras/SlimPajama-627B\")BackgroundToday we are releasing SlimPajama ‚Äì the largest‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/cerebras/SlimPajama-627B."
  },
  {
    "id": "Salesforce/wikitext",
    "name": "Salesforce",
    "parent": "text",
    "color": "#FFD21E",
    "value": 466,
    "downloads_all_time": 21250951,
    "downloads": 693231,
    "likes": 466,
    "description": "Dataset Card for \"wikitext\"Dataset Summary The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia. The dataset is available under the Creative Commons Attribution-ShareAlike License.Compared to the preprocessed version of Penn Treebank (PTB), WikiText-2 is over 2 times larger and WikiText-103 is over110 times larger. The WikiText dataset also features a far larger‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/Salesforce/wikitext."
  },
  {
    "id": "b-mc2/sql-create-context",
    "name": "b-mc2",
    "parent": "text",
    "color": "#FFD21E",
    "value": 466,
    "downloads_all_time": 130633,
    "downloads": 2401,
    "likes": 466,
    "description": "OverviewThis dataset builds from WikiSQL and Spider.There are 78,577 examples of natural language queries, SQL CREATE TABLE statements, and SQL Query answering the question using the CREATE statement as context. This dataset was built with text-to-sql LLMs in mind, intending to prevent hallucination of column and table names often seen when trained on text-to-sql datasets. The CREATE TABLE statement can often be copy and pasted from different DBMS and provides table names, column‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/b-mc2/sql-create-context."
  },
  {
    "id": "nvidia/OpenCodeReasoning",
    "name": "nvidia",
    "parent": "text",
    "color": "#FFD21E",
    "value": 466,
    "downloads_all_time": 30584,
    "downloads": 5364,
    "likes": 466,
    "description": "OpenCodeReasoning: Advancing Data Distillation for Competitive CodingData OverviewOpenCodeReasoning is the largest reasoning-based synthetic dataset to date for coding, comprises 735,255 samples in Python across 28,319 unique competitive programming questions. OpenCodeReasoning is designed for supervised fine-tuning (SFT).Technical Report - Discover the methodology and technical details behind OpenCodeReasoning.Github Repo - Access the complete pipeline used to‚Ä¶ See the full description on the dataset page: https://huggingface.co/datasets/nvidia/OpenCodeReasoning."
  }
]